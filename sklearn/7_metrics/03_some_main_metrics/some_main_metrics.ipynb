{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision\n",
    "\n",
    "$$ PRECISION = \\frac{TP}{TP + FP} $$\n",
    "\n",
    "# Recall\n",
    "\n",
    "$$ RECALL = \\frac{TP}{TP + NF} $$\n",
    "\n",
    "# F-measure\n",
    "### *f1_score in sklearn*\n",
    "\n",
    "$$ F_{MEASURE} = 2 * \\frac{PRECISION * RECALL}{PRECISION + RECALL} $$\n",
    "\n",
    "# Accuracy\n",
    "\n",
    "$$ ACCURACY = \\frac{TP + TN}{TP + TN + FP + FN} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLOT: f1(pre, rec)\n",
    "\n",
    "![g1](f1_1.gif)\n",
    "\n",
    "![g2](f1_2.gif)\n",
    "\n",
    "![g3](f1_3.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dig = load_digits()\n",
    "X = dig.data\n",
    "Y = dig.target == 1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LogReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogReg f1:\t0.89\n",
      "\n",
      "LogReg c_report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.99      0.99      0.99       413\n",
      "       True       0.89      0.89      0.89        37\n",
      "\n",
      "avg / total       0.98      0.98      0.98       450\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "model_pred = model.predict(X_test)\n",
    "model_f1 = f1_score(y_test, model_pred)\n",
    "model_clrep = classification_report(y_test, model_pred)\n",
    "\n",
    "print('LogReg f1:\\t{:.2f}'.format(model_f1), end='\\n\\n')\n",
    "print('LogReg c_report:\\n{}'.format(model_clrep))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC f1:\t\t0.20\n",
      "\n",
      "SVC c_report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.93      1.00      0.96       413\n",
      "       True       1.00      0.11      0.20        37\n",
      "\n",
      "avg / total       0.93      0.93      0.90       450\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = SVC()\n",
    "model.fit(X_train, y_train)\n",
    "model_pred = model.predict(X_test)\n",
    "model_f1 = f1_score(y_test, model_pred)\n",
    "model_clrep = classification_report(y_test, model_pred)\n",
    "\n",
    "print('SVC f1:\\t\\t{:.2f}'.format(model_f1), end='\\n\\n')\n",
    "print('SVC c_report:\\n{}'.format(model_clrep))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DummyC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DummyClassifier f1:\t0.02\n",
      "\n",
      "DummyClassifier c_report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.91      0.86      0.89       413\n",
      "       True       0.02      0.03      0.02        37\n",
      "\n",
      "avg / total       0.84      0.80      0.81       450\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = DummyClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "model_pred = model.predict(X_test)\n",
    "model_f1 = f1_score(y_test, model_pred)\n",
    "model_clrep = classification_report(y_test, model_pred)\n",
    "\n",
    "print('DummyClassifier f1:\\t{:.2f}'.format(model_f1), end='\\n\\n')\n",
    "print('DummyClassifier c_report:\\n{}'.format(model_clrep))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ PrecFalse = \\frac{TN}{TN+FN} $$\n",
    "\n",
    "\n",
    "$$ PrecTrue = \\frac{TP}{TP+FP} $$\n",
    "\n",
    "\n",
    "$$ RecFalse = \\frac{TN}{TN+FP} $$\n",
    "\n",
    "\n",
    "$$ RecTrue = \\frac{TP}{TP+FN} $$\n",
    "\n",
    "\n",
    "$$ F_1False = 2*\\frac{PrecFalse*RecFalse}{PrecFalse + RecFalse} $$\n",
    "\n",
    "\n",
    "$$ F_1True = 2*\\frac{PrecTrue*RecTrue}{PrecTrue + RecTrue} â†’ just f1\\_score $$\n",
    "\n",
    "\n",
    "$$ SupportFalse = Count\\_Of\\_False\\_Classes $$\n",
    "\n",
    "\n",
    "$$ SupportTrue = Count\\_Of\\_True\\_Classes $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![gif](tenor.gif)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
